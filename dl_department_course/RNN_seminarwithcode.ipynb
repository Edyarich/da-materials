{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар по рекуррентным нейронным сетям\n",
    "На этом семинаре мы обучим несколько рекуррентных архитектур для решения задачи сентимент-анализа, то есть предсказания метки тональности предложения.\n",
    "\n",
    "В общем случае рекуррентная нейронная сеть предназначена для обработки последовательности произвольной длины. Однако при реализации метода оказывается проще зафиксировать длину последовательности (даже в pytorch с их динамическими графами :) Мы так и поступим.\n",
    "\n",
    "При выполнении задания вы обучите LSTM с разным уровнем \"коробочности\", а также познакомитесь с различными способами применения дропаута к рекуррентным архитектурам. В рекуррентных архитектурах вариантов, куда можно наложить бинарную маску шума, гораздо больше, чем в нейросетях прямого прохода.\n",
    "\n",
    "Задание сделано так, чтобы его можно было выполнять на CPU, однако RNN - это ресурсоемкая вещь, поэтому на GPU с ними работать приятнее. Можете попробовать использовать [https://colab.research.google.com](https://colab.research.google.com) - бесплатное облако с GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000 \n",
    "index_from = 3\n",
    "n_hidden = 32 # 128\n",
    "n_emb = 32 # 128\n",
    "seq_len = 32 # 200\n",
    "# small network on small data for seminar purposes\n",
    "# after # normal size\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "Функция load_matrix_imdb скачивает матричные данные, перемешивает и загружает их в numpy-массивы.\n",
    "\n",
    "Если у вас не установлен wget, скачайте [архив imdb.npz](https://s3.amazonaws.com/text-datasets/imdb.npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rnn_utils import load_matrix_imdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix_imdb(path='imdb.npz', num_words=None, skip_top=0,\n",
    "              maxlen=None, seed=113,\n",
    "              start_char=1, oov_char=2, index_from=3, **kwargs):\n",
    "    \"\"\"\n",
    "    Modified code from Keras\n",
    "    Loads data matrixes from npz file, crops and pads seqs and returns\n",
    "    shuffled (x_train, y_train), (x_test, y_test)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Downloading matrix data into current folder\")\n",
    "        os.system(\"wget https://s3.amazonaws.com/text-datasets/imdb.npz\")\n",
    "        \n",
    "    with np.load(path, allow_pickle=True) as f:\n",
    "        x_train, labels_train = f['x_train'], f['y_train']\n",
    "        x_test, labels_test = f['x_test'], f['y_test']\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(x_train))\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    labels_train = labels_train[indices]\n",
    "\n",
    "    indices = np.arange(len(x_test))\n",
    "    np.random.shuffle(indices)\n",
    "    x_test = x_test[indices]\n",
    "    labels_test = labels_test[indices]\n",
    "\n",
    "    xs = np.concatenate([x_train, x_test])\n",
    "    labels = np.concatenate([labels_train, labels_test])\n",
    "\n",
    "    if start_char is not None:\n",
    "        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
    "    elif index_from:\n",
    "        xs = [[w + index_from for w in x] for x in xs]\n",
    "\n",
    "    if not num_words:\n",
    "        num_words = max([max(x) for x in xs])\n",
    "    if not maxlen:\n",
    "        maxlen = max([len(x) for x in xs])\n",
    "\n",
    "    # by convention, use 2 as OOV word\n",
    "    # reserve 'index_from' (=3 by default) characters:\n",
    "    # 0 (padding), 1 (start), 2 (OOV)\n",
    "    xs_new = []\n",
    "    for x in xs:\n",
    "        x = x[:maxlen] # crop long sequences\n",
    "        if oov_char is not None: # replace rare or frequent symbols \n",
    "            x = [w if (skip_top <= w < num_words) else oov_char for w in x]\n",
    "        else: # or filter rare and frequent symbols\n",
    "            x = [w for w in x if skip_top <= w < num_words]\n",
    "        x_padded = np.zeros(maxlen)#, dtype = 'int32')\n",
    "        x_padded[-len(x):] = x\n",
    "        xs_new.append(x_padded)    \n",
    "            \n",
    "    idx = len(x_train)\n",
    "    x_train, y_train = np.array(xs_new[:idx]), np.array(labels[:idx])\n",
    "    x_test, y_test = np.array(xs_new[idx:]), np.array(labels[idx:])\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "(X_train, y_train), (X_test, y_test) = load_matrix_imdb(num_words=vocab_size,\n",
    "                                                        maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train) # binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 32), (25000, 32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.000e+00, 1.400e+01, 2.200e+01, 1.600e+01, 4.300e+01, 5.300e+02,\n",
       "       9.730e+02, 1.622e+03, 1.385e+03, 6.500e+01, 4.580e+02, 4.468e+03,\n",
       "       6.600e+01, 3.941e+03, 4.000e+00, 1.730e+02, 3.600e+01, 2.560e+02,\n",
       "       5.000e+00, 2.500e+01, 1.000e+02, 4.300e+01, 8.380e+02, 1.120e+02,\n",
       "       5.000e+01, 6.700e+02, 2.000e+00, 9.000e+00, 3.500e+01, 4.800e+02,\n",
       "       2.840e+02, 5.000e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # sequence of coded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.long), \n",
    "                               torch.tensor(y_train, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long), \n",
    "                               torch.tensor(y_test, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка и обучение RNN в pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша нейросеть будет обрабатывать входную последовательность по словам (word level). Мы будем использовать простую и стандарную рекуррентную архитектуру для сентимент-анализа: слой представлений, слой LSTM и полносвязный слой, предсказывающий выход по последнему скрытому состоянию.\n",
    "\n",
    "Ниже дан код для сборки и обучения нашей нейросети. Обратите внимание на ### pay attention here, указывающие на особенности кода при использовании рекуррентных слоев. \n",
    "\n",
    "Чтобы потом было удобно проводить сравнение времени работы разных моделей, запустите код ниже и измерьте время обучения на вашей системе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, \\\n",
    "                 batch_size, rec_layer=nn.LSTM, embedding=nn.Embedding, \\\n",
    "                 dropout=None):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.word_embeddings = embedding(vocab_size, embedding_dim)\n",
    "        if dropout:\n",
    "            self.rnn = rec_layer(embedding_dim, hidden_dim, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = rec_layer(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        embedding = self.word_embeddings(sentences)\n",
    "        out, hidden = self.rnn(embedding) # pay attention here!\n",
    "        res = self.hidden2label(out[-1])\n",
    "        return torch.sigmoid(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Исходный код LSTM](http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassifier(embedding_dim=n_emb,\n",
    "                       hidden_dim=n_hidden,\n",
    "                       vocab_size=vocab_size,\n",
    "                       label_size=1,\n",
    "                       batch_size=batch_size, \n",
    "                       rec_layer=nn.LSTM,\n",
    "                       dropout=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "lossfun = nn.BCELoss(reduction='sum') + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, lossfun, optimizer, device):\n",
    "    model.train()\n",
    "    for it, traindata in enumerate(train_loader):\n",
    "        train_inputs, train_labels = traindata\n",
    "        train_inputs = train_inputs.to(device) \n",
    "        train_labels = train_labels.to(device)\n",
    "        train_labels = torch.squeeze(train_labels)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        output = model(train_inputs.t()) # pay attention here!\n",
    "\n",
    "        loss = lossfun(output.view(-1), train_labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(loader, model, lossfun, device):\n",
    "    model.eval()\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    for it, data in enumerate(loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        labels = torch.squeeze(labels)\n",
    "\n",
    "        output = model(inputs.t()) # pay attention here!\n",
    "        loss = lossfun(output.view(-1), labels.float()) + torch.norm(WW^T - I)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # calc testing acc        \n",
    "        pred = output.view(-1) > 0.5\n",
    "        correct = pred == labels.byte()\n",
    "        total_acc += torch.sum(correct).item() / len(correct)\n",
    "\n",
    "    total = it + 1\n",
    "    return total_loss / total, total_acc / total\n",
    "    \n",
    "\n",
    "def train(train_loader, test_loader, model, lossfun, optimizer, \\\n",
    "          device, num_epochs):\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(train_loader, model, lossfun, optimizer, device)\n",
    "        train_loss, train_acc = evaluate(train_loader, model, lossfun, device)\n",
    "        train_loss_.append(train_loss)\n",
    "        train_acc_.append(train_acc)\n",
    "        test_loss, test_acc = evaluate(test_loader, model, lossfun, device)\n",
    "        test_loss_.append(test_loss)\n",
    "        test_acc_.append(test_acc)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:3d}/{num_epochs:3d} '\n",
    "              f'Training Loss: {train_loss_[epoch]:.3f}, Testing Loss: {test_loss_[epoch]:.3f}, '\n",
    "              f'Training Acc: {train_acc_[epoch]:.3f}, Testing Acc: {test_acc_[epoch]:.3f}')\n",
    "\n",
    "    return train_loss_, train_acc_, test_loss_, test_acc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/ 30 Training Loss: 83.852, Testing Loss: 85.257, Training Acc: 0.619, Testing Acc: 0.598\n",
      "Epoch:   2/ 30 Training Loss: 75.851, Testing Loss: 80.369, Training Acc: 0.682, Testing Acc: 0.645\n",
      "Epoch:   3/ 30 Training Loss: 69.847, Testing Loss: 77.532, Training Acc: 0.730, Testing Acc: 0.665\n",
      "Epoch:   4/ 30 Training Loss: 65.427, Testing Loss: 75.966, Training Acc: 0.752, Testing Acc: 0.676\n",
      "Epoch:   5/ 30 Training Loss: 61.612, Testing Loss: 77.338, Training Acc: 0.770, Testing Acc: 0.686\n",
      "Epoch:   6/ 30 Training Loss: 58.236, Testing Loss: 76.019, Training Acc: 0.784, Testing Acc: 0.693\n",
      "Epoch:   7/ 30 Training Loss: 52.706, Testing Loss: 73.616, Training Acc: 0.815, Testing Acc: 0.702\n",
      "Epoch:   8/ 30 Training Loss: 48.887, Testing Loss: 76.414, Training Acc: 0.829, Testing Acc: 0.704\n",
      "Epoch:   9/ 30 Training Loss: 46.773, Testing Loss: 75.653, Training Acc: 0.844, Testing Acc: 0.705\n",
      "Epoch:  10/ 30 Training Loss: 42.157, Testing Loss: 80.193, Training Acc: 0.860, Testing Acc: 0.706\n",
      "Epoch:  11/ 30 Training Loss: 39.156, Testing Loss: 84.401, Training Acc: 0.871, Testing Acc: 0.707\n",
      "Epoch:  12/ 30 Training Loss: 36.537, Testing Loss: 84.077, Training Acc: 0.884, Testing Acc: 0.703\n",
      "Epoch:  13/ 30 Training Loss: 35.306, Testing Loss: 83.051, Training Acc: 0.893, Testing Acc: 0.704\n",
      "Epoch:  14/ 30 Training Loss: 32.130, Testing Loss: 89.640, Training Acc: 0.900, Testing Acc: 0.701\n",
      "Epoch:  15/ 30 Training Loss: 29.274, Testing Loss: 92.546, Training Acc: 0.912, Testing Acc: 0.705\n",
      "Epoch:  16/ 30 Training Loss: 26.206, Testing Loss: 99.872, Training Acc: 0.921, Testing Acc: 0.703\n",
      "Epoch:  17/ 30 Training Loss: 25.196, Testing Loss: 99.402, Training Acc: 0.926, Testing Acc: 0.702\n",
      "Epoch:  18/ 30 Training Loss: 25.760, Testing Loss: 104.458, Training Acc: 0.921, Testing Acc: 0.700\n",
      "Epoch:  19/ 30 Training Loss: 20.696, Testing Loss: 113.225, Training Acc: 0.941, Testing Acc: 0.698\n",
      "Epoch:  20/ 30 Training Loss: 18.143, Testing Loss: 122.245, Training Acc: 0.948, Testing Acc: 0.703\n",
      "Epoch:  21/ 30 Training Loss: 16.122, Testing Loss: 126.775, Training Acc: 0.956, Testing Acc: 0.698\n",
      "Epoch:  22/ 30 Training Loss: 14.745, Testing Loss: 131.241, Training Acc: 0.960, Testing Acc: 0.697\n",
      "Epoch:  23/ 30 Training Loss: 13.548, Testing Loss: 135.817, Training Acc: 0.964, Testing Acc: 0.697\n",
      "Epoch:  24/ 30 Training Loss: 13.140, Testing Loss: 147.236, Training Acc: 0.964, Testing Acc: 0.692\n",
      "Epoch:  25/ 30 Training Loss: 10.686, Testing Loss: 156.184, Training Acc: 0.972, Testing Acc: 0.695\n",
      "Epoch:  26/ 30 Training Loss: 9.409, Testing Loss: 151.327, Training Acc: 0.978, Testing Acc: 0.692\n",
      "Epoch:  27/ 30 Training Loss: 9.642, Testing Loss: 151.623, Training Acc: 0.978, Testing Acc: 0.691\n",
      "Epoch:  28/ 30 Training Loss: 7.456, Testing Loss: 163.595, Training Acc: 0.985, Testing Acc: 0.689\n",
      "Epoch:  29/ 30 Training Loss: 6.422, Testing Loss: 172.485, Training Acc: 0.987, Testing Acc: 0.689\n",
      "Epoch:  30/ 30 Training Loss: 6.142, Testing Loss: 180.913, Training Acc: 0.987, Testing Acc: 0.688\n",
      "CPU times: user 52.1 s, sys: 17.3 s, total: 1min 9s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%time a, b, c, d = train(train_loader, test_loader, model, lossfun, \\\n",
    "                   optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нерегуляризованные LSTM часто быстро переобучаются (и мы это видим по точности на контроле). Чтобы с этим бороться, часто используют L2-регуляризацию и дропаут.\n",
    "Однако способов накладывать дропаут на рекуррентный слой достаточно много, и далеко не все хорошо работают. По [ссылке](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b) доступен хороший обзор дропаутов для RNN.\n",
    "\n",
    "Мы реализуем два варианта дропаута для RNN (и третий дополнительно). Заодно увидим, что для реализации различных усовершенствований рекуррентной архитектуры приходится \"вскрывать\" слой до различной \"глубины\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация дропаута по статье Гала и Гарамани\n",
    "Начнем с дропаута, описанного в [статье Гала и Гарамани](https://arxiv.org/abs/1512.05287).\n",
    "Для этого нам потребуется перейти от использования слоя nn.LSTM, полностью скрывающего от нас рекуррентную логику, к использованию слоя nn.LSTMCell, обрабатывающего лишь один временной шаг нашей последовательности (а всю логику вокруг придется реализовать самостоятельно). \n",
    "\n",
    "Допишите класс RNNLayer. При dropout=0 ваш класс должен работать как обычный слой LSTM, а при dropout > 0 накладывать бинарную маску на входной и скрытый вектор на каждом временном шаге, причем эта маска должна быть одинаковой во все моменты времени.\n",
    "\n",
    "Дропаут Гала и Гарамани в виде формул (m обознаает маску дропаута):\n",
    "$$\n",
    "h_{t-1} = h_{t-1}*m_h, \\, x_t = x_t * m_x\n",
    "$$\n",
    "(далее обычный шаг рекуррентной архитектуры, например LSTM)\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t) \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_h0_c0(num_objects, hidden_size, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    return h0 and c0, use some_existing_tensor.new_zeros() to gen them\n",
    "    h0 shape: num_objects x hidden_size\n",
    "    c0 shape: num_objects x hidden_size\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    h0 = some_existing_tensor.new_zeros((num_objects, hidden_size))\n",
    "    c0 = some_existing_tensor.new_zeros((num_objects, hidden_size))\n",
    "    return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dropout_mask(input_size, hidden_size, is_training, p, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    is_training: if True, gen masks from Bernoulli\n",
    "                 if False, gen masks consisting of (1-p)\n",
    "    \n",
    "    return dropout masks of size input_size, hidden_size if p is not None\n",
    "    return one masks if p is None\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    mask0 = some_existing_tensor.new_ones(input_size)\n",
    "    mask1 = some_existing_tensor.new_ones(hidden_size)\n",
    "    if p:\n",
    "        if is_training:\n",
    "            mask0 *= p\n",
    "            mask0 = torch.bernoulli(mask0)\n",
    "            mask1 *= p\n",
    "            mask1 = torch.bernoulli(mask1)\n",
    "        else:\n",
    "            mask0 *= (1-p)\n",
    "            #mask0 = torch.bernoulli_(mask0)\n",
    "            mask1 *= (1-p)\n",
    "            #mask1 = torch.bernoulli_(mask1)\n",
    "    return mask0, mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=None):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.dropout = dropout\n",
    "        self.rnn_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # initialize h_0, c_0\n",
    "        h_0, c_0 = init_h0_c0(inp.shape[1], self.hidden_size, inp)\n",
    "        \n",
    "        # gen masks\n",
    "        input_mask, hidden_mask = gen_dropout_mask(self.input_size, \\\n",
    "                                                   self.hidden_size, \\\n",
    "                                                   self.training, \\\n",
    "                                                   self.dropout, \\\n",
    "                                                   inp)\n",
    "        \n",
    "        \n",
    "        ### your code here\n",
    "        ### implement recurrent logic and return what nn.LSTM returns\n",
    "        ### do not forget to apply generated dropout masks!\n",
    "        out = []\n",
    "        h, c = h_0, c_0\n",
    "        for i in range(inp.shape[0]):\n",
    "            h, c = self.rnn_cell(inp[i]*input_mask, (h*hidden_mask, c))\n",
    "            out.append(h)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте реализованную модель с выключенным дропаутом (слой RNNLayer надо передать в RNNClassifier в качестве rec_layer). Замерьте время обучения (%time). Сильно ли оно увеличилось по сравнению с nn.LSTM (LSTM \"из коробки\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/ 30 Training Loss: 84.288, Testing Loss: 85.622, Training Acc: 0.613, Testing Acc: 0.589\n",
      "Epoch:   2/ 30 Training Loss: 76.006, Testing Loss: 80.819, Training Acc: 0.685, Testing Acc: 0.643\n",
      "Epoch:   3/ 30 Training Loss: 70.186, Testing Loss: 76.887, Training Acc: 0.729, Testing Acc: 0.667\n",
      "Epoch:   4/ 30 Training Loss: 64.336, Testing Loss: 75.524, Training Acc: 0.755, Testing Acc: 0.684\n",
      "Epoch:   5/ 30 Training Loss: 60.038, Testing Loss: 74.052, Training Acc: 0.781, Testing Acc: 0.692\n",
      "Epoch:   6/ 30 Training Loss: 55.509, Testing Loss: 74.238, Training Acc: 0.798, Testing Acc: 0.703\n",
      "Epoch:   7/ 30 Training Loss: 53.868, Testing Loss: 76.738, Training Acc: 0.807, Testing Acc: 0.703\n",
      "Epoch:   8/ 30 Training Loss: 48.496, Testing Loss: 75.073, Training Acc: 0.831, Testing Acc: 0.709\n",
      "Epoch:   9/ 30 Training Loss: 46.116, Testing Loss: 77.050, Training Acc: 0.842, Testing Acc: 0.708\n",
      "Epoch:  10/ 30 Training Loss: 42.689, Testing Loss: 77.346, Training Acc: 0.858, Testing Acc: 0.711\n",
      "Epoch:  11/ 30 Training Loss: 39.974, Testing Loss: 78.985, Training Acc: 0.870, Testing Acc: 0.713\n",
      "Epoch:  12/ 30 Training Loss: 37.746, Testing Loss: 85.693, Training Acc: 0.876, Testing Acc: 0.708\n",
      "Epoch:  13/ 30 Training Loss: 33.879, Testing Loss: 86.452, Training Acc: 0.893, Testing Acc: 0.713\n",
      "Epoch:  14/ 30 Training Loss: 31.901, Testing Loss: 86.280, Training Acc: 0.903, Testing Acc: 0.710\n",
      "Epoch:  15/ 30 Training Loss: 30.098, Testing Loss: 90.946, Training Acc: 0.909, Testing Acc: 0.703\n",
      "Epoch:  16/ 30 Training Loss: 27.032, Testing Loss: 93.047, Training Acc: 0.921, Testing Acc: 0.709\n",
      "Epoch:  17/ 30 Training Loss: 24.189, Testing Loss: 101.515, Training Acc: 0.929, Testing Acc: 0.706\n",
      "Epoch:  18/ 30 Training Loss: 23.337, Testing Loss: 100.756, Training Acc: 0.935, Testing Acc: 0.702\n",
      "Epoch:  19/ 30 Training Loss: 19.582, Testing Loss: 117.896, Training Acc: 0.944, Testing Acc: 0.709\n",
      "Epoch:  20/ 30 Training Loss: 18.333, Testing Loss: 125.530, Training Acc: 0.948, Testing Acc: 0.707\n",
      "Epoch:  21/ 30 Training Loss: 16.788, Testing Loss: 122.227, Training Acc: 0.956, Testing Acc: 0.701\n",
      "Epoch:  22/ 30 Training Loss: 14.546, Testing Loss: 133.079, Training Acc: 0.961, Testing Acc: 0.699\n",
      "Epoch:  23/ 30 Training Loss: 13.324, Testing Loss: 143.936, Training Acc: 0.966, Testing Acc: 0.698\n",
      "Epoch:  24/ 30 Training Loss: 11.970, Testing Loss: 150.595, Training Acc: 0.969, Testing Acc: 0.698\n",
      "Epoch:  25/ 30 Training Loss: 9.923, Testing Loss: 159.897, Training Acc: 0.976, Testing Acc: 0.699\n",
      "Epoch:  26/ 30 Training Loss: 9.301, Testing Loss: 164.496, Training Acc: 0.977, Testing Acc: 0.694\n",
      "Epoch:  27/ 30 Training Loss: 9.315, Testing Loss: 154.912, Training Acc: 0.980, Testing Acc: 0.691\n",
      "Epoch:  28/ 30 Training Loss: 7.362, Testing Loss: 170.875, Training Acc: 0.984, Testing Acc: 0.696\n",
      "Epoch:  29/ 30 Training Loss: 6.699, Testing Loss: 179.310, Training Acc: 0.986, Testing Acc: 0.696\n",
      "Epoch:  30/ 30 Training Loss: 5.496, Testing Loss: 194.327, Training Acc: 0.989, Testing Acc: 0.691\n",
      "CPU times: user 2min 36s, sys: 17.6 s, total: 2min 54s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "model = RNNClassifier(embedding_dim=n_emb,\n",
    "                       hidden_dim=n_hidden,\n",
    "                       vocab_size=vocab_size,\n",
    "                       label_size=1,\n",
    "                       batch_size=batch_size, \n",
    "                       rec_layer=RNNLayer,\n",
    "                       dropout=None).to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "lossfun = nn.BCELoss(reduction='sum')\n",
    "\n",
    "%time a, b, c, d = train(train_loader, test_loader, model, lossfun, \\\n",
    "                   optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте полученную модель c dropout=0.5, вновь замерив время обучения. Получилось ли побороть переобучение? Сильно ли дольше обучается данная модель по сравнению с предыдущей? (доп. время тратится на генерацию масок дропаута)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/ 30 Training Loss: 88.213, Testing Loss: 88.323, Training Acc: 0.520, Testing Acc: 0.518\n",
      "Epoch:   2/ 30 Training Loss: 87.817, Testing Loss: 88.098, Training Acc: 0.552, Testing Acc: 0.532\n",
      "Epoch:   3/ 30 Training Loss: 86.641, Testing Loss: 87.347, Training Acc: 0.584, Testing Acc: 0.556\n",
      "Epoch:   4/ 30 Training Loss: 84.679, Testing Loss: 86.047, Training Acc: 0.614, Testing Acc: 0.580\n",
      "Epoch:   5/ 30 Training Loss: 83.256, Testing Loss: 85.047, Training Acc: 0.629, Testing Acc: 0.598\n",
      "Epoch:   6/ 30 Training Loss: 81.011, Testing Loss: 83.550, Training Acc: 0.652, Testing Acc: 0.613\n",
      "Epoch:   7/ 30 Training Loss: 79.690, Testing Loss: 82.667, Training Acc: 0.668, Testing Acc: 0.627\n",
      "Epoch:   8/ 30 Training Loss: 77.882, Testing Loss: 81.628, Training Acc: 0.683, Testing Acc: 0.636\n",
      "Epoch:   9/ 30 Training Loss: 75.990, Testing Loss: 80.691, Training Acc: 0.692, Testing Acc: 0.646\n",
      "Epoch:  10/ 30 Training Loss: 74.648, Testing Loss: 79.660, Training Acc: 0.708, Testing Acc: 0.657\n",
      "Epoch:  11/ 30 Training Loss: 73.263, Testing Loss: 79.122, Training Acc: 0.713, Testing Acc: 0.662\n",
      "Epoch:  12/ 30 Training Loss: 71.601, Testing Loss: 78.218, Training Acc: 0.725, Testing Acc: 0.670\n",
      "Epoch:  13/ 30 Training Loss: 71.625, Testing Loss: 78.289, Training Acc: 0.723, Testing Acc: 0.669\n",
      "Epoch:  14/ 30 Training Loss: 69.329, Testing Loss: 77.249, Training Acc: 0.741, Testing Acc: 0.677\n",
      "Epoch:  15/ 30 Training Loss: 68.129, Testing Loss: 76.538, Training Acc: 0.742, Testing Acc: 0.683\n",
      "Epoch:  16/ 30 Training Loss: 65.570, Testing Loss: 75.483, Training Acc: 0.757, Testing Acc: 0.690\n",
      "Epoch:  17/ 30 Training Loss: 65.397, Testing Loss: 75.033, Training Acc: 0.761, Testing Acc: 0.691\n",
      "Epoch:  18/ 30 Training Loss: 63.213, Testing Loss: 74.233, Training Acc: 0.769, Testing Acc: 0.698\n",
      "Epoch:  19/ 30 Training Loss: 63.304, Testing Loss: 74.155, Training Acc: 0.772, Testing Acc: 0.700\n",
      "Epoch:  20/ 30 Training Loss: 61.460, Testing Loss: 73.419, Training Acc: 0.776, Testing Acc: 0.704\n",
      "Epoch:  21/ 30 Training Loss: 60.564, Testing Loss: 72.957, Training Acc: 0.783, Testing Acc: 0.709\n",
      "Epoch:  22/ 30 Training Loss: 59.596, Testing Loss: 72.404, Training Acc: 0.791, Testing Acc: 0.708\n",
      "Epoch:  23/ 30 Training Loss: 58.645, Testing Loss: 71.853, Training Acc: 0.794, Testing Acc: 0.712\n",
      "Epoch:  24/ 30 Training Loss: 56.517, Testing Loss: 71.338, Training Acc: 0.801, Testing Acc: 0.716\n",
      "Epoch:  25/ 30 Training Loss: 56.572, Testing Loss: 71.277, Training Acc: 0.804, Testing Acc: 0.714\n",
      "Epoch:  26/ 30 Training Loss: 55.070, Testing Loss: 70.805, Training Acc: 0.809, Testing Acc: 0.719\n",
      "Epoch:  27/ 30 Training Loss: 55.278, Testing Loss: 71.258, Training Acc: 0.808, Testing Acc: 0.720\n",
      "Epoch:  28/ 30 Training Loss: 54.736, Testing Loss: 71.651, Training Acc: 0.807, Testing Acc: 0.719\n",
      "Epoch:  29/ 30 Training Loss: 53.028, Testing Loss: 70.302, Training Acc: 0.820, Testing Acc: 0.722\n",
      "Epoch:  30/ 30 Training Loss: 52.021, Testing Loss: 70.119, Training Acc: 0.823, Testing Acc: 0.723\n",
      "CPU times: user 2min 32s, sys: 16.7 s, total: 2min 49s\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "model = RNNClassifier(embedding_dim=n_emb,\n",
    "                       hidden_dim=n_hidden,\n",
    "                       vocab_size=vocab_size,\n",
    "                       label_size=1,\n",
    "                       batch_size=batch_size, \n",
    "                       rec_layer=RNNLayer,\n",
    "                       dropout=0.5).to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "lossfun = nn.BCELoss(reduction='sum')\n",
    "\n",
    "%time a, b, c, d = train(train_loader, test_loader, model, lossfun, \\\n",
    "                   optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (word_embeddings): Embedding(20000, 32)\n",
       "  (rnn): FastRNNLayer(\n",
       "    (module): LSTM(32, 32)\n",
       "  )\n",
       "  (hidden2label): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация дропаута по статье Гала и Гарамани. Дубль 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<начало взлома pytorch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При разворачивании цикла по времени средствами python обучение рекуррентной нейросети сильно замедляется. Однако для реализации дропаута Гала и Гарамани необязательно явно задавать в коде домножение нейронов на маски. Можно схитрить и обойтись использованием слоя nn.LSTM: перед вызовом forward слоя nn.LSTM подменять его веса на веса, домноженные по строкам на маски. А обучаемые веса хранить отдельно. Именно так этот дропаут реализован в библиотеке fastai, код из которой использован в ячейке ниже.\n",
    "\n",
    "Такой слой реализуется в виде обертки над nn.LSTM. Допишите класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0):\n",
    "        super(FastRNNLayer, self).__init__()\n",
    "        self.module = nn.LSTM(input_size, hidden_size)\n",
    "        self.dropout = dropout\n",
    "        self.layer_names = ['weight_hh_l0', 'weight_ih_l0']\n",
    "        for layer in self.layer_names:\n",
    "            # Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            \n",
    "    def _setweights(self):\n",
    "        \"Apply dropout to the raw weights.\"\n",
    "        ### your code here\n",
    "        ### generate input_mask and hidden_mask (use function gen_dropout_mask)\n",
    "        input_mask, hidden_mask = gen_dropout_mask(self.module.input_size, \\\n",
    "                                                   self.module.hidden_size, \\\n",
    "                                                   self.training, \\\n",
    "                                                   self.dropout, \\\n",
    "                                                   self.module.weight_hh_l0.data)\n",
    "        for layer, mask in zip(self.layer_names, (hidden_mask, input_mask)):\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer].data = raw_w * mask\n",
    "\n",
    "    def forward(self, *args):\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            \n",
    "            ### your code here\n",
    "            ### set new weights of self.module and call its forward\n",
    "            self._setweights()\n",
    "            return self.module.forward(*args)\n",
    "\n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте полученный слой (вновь подставив его в RNNClassifier в качестве rec_layer) с dropout=0.5. Сравните время обучения с предыдущими моделями. Проследите, чтобы качество получилось такое же, как при первой реализации этого дропаута."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/ 30 Training Loss: 88.250, Testing Loss: 88.349, Training Acc: 0.527, Testing Acc: 0.515\n",
      "Epoch:   2/ 30 Training Loss: 88.135, Testing Loss: 88.292, Training Acc: 0.533, Testing Acc: 0.522\n",
      "Epoch:   3/ 30 Training Loss: 87.975, Testing Loss: 88.223, Training Acc: 0.545, Testing Acc: 0.524\n",
      "Epoch:   4/ 30 Training Loss: 87.784, Testing Loss: 88.133, Training Acc: 0.550, Testing Acc: 0.528\n",
      "Epoch:   5/ 30 Training Loss: 87.516, Testing Loss: 88.000, Training Acc: 0.559, Testing Acc: 0.533\n",
      "Epoch:   6/ 30 Training Loss: 87.148, Testing Loss: 87.846, Training Acc: 0.568, Testing Acc: 0.539\n",
      "Epoch:   7/ 30 Training Loss: 86.666, Testing Loss: 87.623, Training Acc: 0.580, Testing Acc: 0.546\n",
      "Epoch:   8/ 30 Training Loss: 85.961, Testing Loss: 87.302, Training Acc: 0.593, Testing Acc: 0.554\n",
      "Epoch:   9/ 30 Training Loss: 85.098, Testing Loss: 86.898, Training Acc: 0.604, Testing Acc: 0.565\n",
      "Epoch:  10/ 30 Training Loss: 83.855, Testing Loss: 86.247, Training Acc: 0.620, Testing Acc: 0.576\n",
      "Epoch:  11/ 30 Training Loss: 82.490, Testing Loss: 85.499, Training Acc: 0.634, Testing Acc: 0.587\n",
      "Epoch:  12/ 30 Training Loss: 80.405, Testing Loss: 84.276, Training Acc: 0.654, Testing Acc: 0.605\n",
      "Epoch:  13/ 30 Training Loss: 78.488, Testing Loss: 83.067, Training Acc: 0.671, Testing Acc: 0.620\n",
      "Epoch:  14/ 30 Training Loss: 76.700, Testing Loss: 81.984, Training Acc: 0.684, Testing Acc: 0.631\n",
      "Epoch:  15/ 30 Training Loss: 74.456, Testing Loss: 80.673, Training Acc: 0.700, Testing Acc: 0.643\n",
      "Epoch:  16/ 30 Training Loss: 72.460, Testing Loss: 79.582, Training Acc: 0.711, Testing Acc: 0.653\n",
      "Epoch:  17/ 30 Training Loss: 70.995, Testing Loss: 78.621, Training Acc: 0.723, Testing Acc: 0.660\n",
      "Epoch:  18/ 30 Training Loss: 69.556, Testing Loss: 77.944, Training Acc: 0.729, Testing Acc: 0.666\n",
      "Epoch:  19/ 30 Training Loss: 67.639, Testing Loss: 77.043, Training Acc: 0.742, Testing Acc: 0.674\n",
      "Epoch:  20/ 30 Training Loss: 66.268, Testing Loss: 76.345, Training Acc: 0.750, Testing Acc: 0.678\n",
      "Epoch:  21/ 30 Training Loss: 64.957, Testing Loss: 75.765, Training Acc: 0.757, Testing Acc: 0.684\n",
      "Epoch:  22/ 30 Training Loss: 63.739, Testing Loss: 75.349, Training Acc: 0.762, Testing Acc: 0.688\n",
      "Epoch:  23/ 30 Training Loss: 62.511, Testing Loss: 74.684, Training Acc: 0.769, Testing Acc: 0.692\n",
      "Epoch:  24/ 30 Training Loss: 61.343, Testing Loss: 74.241, Training Acc: 0.775, Testing Acc: 0.696\n",
      "Epoch:  25/ 30 Training Loss: 60.551, Testing Loss: 73.971, Training Acc: 0.776, Testing Acc: 0.696\n",
      "Epoch:  26/ 30 Training Loss: 59.519, Testing Loss: 73.580, Training Acc: 0.782, Testing Acc: 0.700\n",
      "Epoch:  27/ 30 Training Loss: 58.600, Testing Loss: 73.380, Training Acc: 0.788, Testing Acc: 0.704\n",
      "Epoch:  28/ 30 Training Loss: 57.672, Testing Loss: 73.050, Training Acc: 0.790, Testing Acc: 0.707\n",
      "Epoch:  29/ 30 Training Loss: 56.720, Testing Loss: 72.976, Training Acc: 0.795, Testing Acc: 0.709\n",
      "Epoch:  30/ 30 Training Loss: 55.885, Testing Loss: 72.815, Training Acc: 0.798, Testing Acc: 0.710\n",
      "CPU times: user 53.2 s, sys: 16.8 s, total: 1min 10s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "model = RNNClassifier(embedding_dim=n_emb,\n",
    "                       hidden_dim=n_hidden,\n",
    "                       vocab_size=vocab_size,\n",
    "                       label_size=1,\n",
    "                       batch_size=batch_size, \n",
    "                       rec_layer=FastRNNLayer,\n",
    "                       dropout=0.5).to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "lossfun = nn.BCELoss(reduction='sum')\n",
    "\n",
    "%time a, b, c, d = train(train_loader, test_loader, model, lossfun, \\\n",
    "                   optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</конец взлома pytorch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация дропаута по статье Семениуты и др\n",
    "Перейдем к реализации дропаута для LSTM по статье [Semeniuta et al](http://www.aclweb.org/anthology/C16-1165). \n",
    "\n",
    "Этот метод применения дропаута не менее популярен, чем предыдущий. Его особенность состоит в том, что он придуман специально для гейтовых архитектур. В контексте LSTM этот дропаут накладывается только на информационный поток (m_h - маска дропаута):\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot g \\odot {\\bf m_h} \\quad\n",
    "h_t =  o \\odot tanh(c_t) \\nonumber\n",
    "$$\n",
    "На входы $x_t$ маска накладывается как в предыдущем дропауте. Впрочем, на входы маску можно наложить вообще до вызова рекуррентного слоя.\n",
    "\n",
    "Согласно статье, маска дропаута может быть как одинаковая, так и разная для всех моментов времени. Мы сделаем одинаковую.\n",
    "\n",
    "Для реализации этого дропаута можно: а) самостоятельно реализовать LSTM (интерфейса LSTMCell не хватит) б) снова воспользоваться трюком с установкой весов (но тут мы опираемся на свойство hanh(0)=0, к тому же, трюк в данном случае выглядит менее тривиально, чем с дропаутом Гала). Предлагается реализовать дрпоаут по сценираю а. Допишите класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HandmadeLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.):\n",
    "        super(HandmadeLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.input_weights = nn.Linear(input_size, 4 * hidden_size)\n",
    "        self.hidden_weights = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        \n",
    "        self.reset_params()\n",
    "\n",
    "\n",
    "    def reset_params(self):\n",
    "        \"\"\"\n",
    "        initialization as in Pytorch\n",
    "        do not forget to call this method!\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(0, stdv)\n",
    "            \n",
    "\n",
    "    def forward(self, inp, hidden=None):\n",
    "        ### your code here\n",
    "        # use functions init_h0_c0 and gen_dropout_masks defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте вашу реализацию без дропаута (проконтролируйте качество и сравните время обучения с временем обучения nn.LSTM и RNNLayer), а также с dropout=0.5. Сравните качество модели с таким дропаутом с качеством модели с дропаутом Гала и Гарамани."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zoneout\n",
    "Это еще одна модификация идеи дропаута применительно к рекуррентным нейросетям. В Zoneout на каждом временном шаге с вероятностью p компонента скрытого состояния обновляется, а с вероятностью 1-p берется с предыдущего шага. \n",
    "В Виде формул (m^t_h - бинарная маска):\n",
    " \n",
    "(сначала обычный рекуррентный переход, например LSTM)\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t) \\nonumber\n",
    "$$\n",
    "Затем Zoneout:\n",
    "$$\n",
    "h_t = h_t * m_h^t + h_{t-1}*(1-m_h^t)\n",
    "$$\n",
    "В этом методе маска уже должна быть разная во все моменты времени (иначе метод упрощается до дропаута Гала и Гарамани). На входы $x_t$ вновь можно накладывать маску до начала работы рекуррентного слоя.  \n",
    "\n",
    "Если у вас осталось время, вы можете реализовать этот метод. Выберите основу из трех рассмотренных случаев самостоятельно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "313px",
    "left": "926px",
    "right": "27px",
    "top": "120px",
    "width": "343px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
