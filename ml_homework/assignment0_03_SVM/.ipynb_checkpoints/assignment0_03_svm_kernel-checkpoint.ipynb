{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels for hand-made SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this homework we will again look at SVM kernels and will write and test rbf kernel for our own implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you are using Google Colab, uncomment the next line to download `svm.py`\n",
    "You can open and change downloaded `.py` files in Colab using the \"Files\" sidebar on the left.\n",
    "'''\n",
    "# !wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_basic/homeworks_basic/assignment0_03_SVM/svm.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for our SVM class\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 50)\n",
    "    y = np.linspace(ylim[0], ylim[1], 50)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.predict(xy).reshape(X.shape)\n",
    "    # plot decision boundary and margins\n",
    "    CS = ax.contourf(X, Y, P, origin='lower', cmap='autumn', alpha=0.1)\n",
    "    plt.colorbar(CS, ax=ax, shrink=0.8, extend='both')\n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "\n",
    "X, y = make_circles(150, factor=.1, noise=.1, random_state=42)\n",
    "\n",
    "X_test, y_test = X[100:], y[100:]\n",
    "X, y = X[:100], y[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn realization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf').fit(X, y)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at our realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to open svm.py file and add all missed lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM __primal__ optimization problem can be formulated as\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - y_i (w X_i - b)) + \\lambda ||w||_2 \\to \\min_w $$\n",
    "\n",
    "This problem can be solved with gradient or sub-gradien methods.\n",
    "\n",
    "-----\n",
    "Whereas __dual__ optimization problem formulates as follows:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n c_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n y_i c_i (X_i \\cdot X_j ) y_j c_j \\to \\max_{c_1,...,c_n} \\\\ \\text{subject to} \\\\\n",
    "\\sum_{i=1}^n c_iy_i=0 \\\\\n",
    "0 \\leq c_i \\leq \\frac{1}{2n\\lambda} \\forall i\n",
    "$$\n",
    "\n",
    "Where $W = \\sum_{i=1}^n c_i y_i X_i$.\n",
    "\n",
    "In this quadratic optimization problem we can use kernel trick: <br/>\n",
    "introduce fucntion $K(X_i, X_j) = \\phi (X_i) \\phi (X_j)$ and change dot products in our optimization problem\n",
    "\n",
    "Then we have \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n c_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n y_i c_i K(X_i, X_j) y_j c_j \\to \\max_{c_1,...,c_n} \\\\\n",
    "\\text{subject to} \\\\\n",
    "\\sum_{i=1}^n c_iy_i=0 \\\\\n",
    "0 \\leq c_i \\leq \\frac{1}{2n\\lambda} \\forall i\n",
    "$$\n",
    "\n",
    "$W = \\sum_{i=1}^n c_i y_i \\phi(X_i)$\n",
    "\n",
    "In quadratic programming we can straightforwardly add kernel function, but it is not that simple, if we want to use gradient algorithms.\n",
    "\n",
    "----\n",
    "However primal optimization problem with kernels can be formulated like (see [Olivier Chapelle, 2006](https://www.cs.utah.edu/~piyush/teaching/svm-solving-primal.pdf)):\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^n \\beta_i K(x_i, x)$$\n",
    "\n",
    "$$K: K_{i,j} = K(x_i, x_j)$$\n",
    "\n",
    "$$ \\lambda \\vec{\\beta^T} K \\vec{\\beta} + \\sum_{i=1}^n L(y_i, K_i^T \\vec{\\beta}) \\to \\min_{\\vec{\\beta}}$$\n",
    "\n",
    "where L is Hinge loss: $L(y_i, K_i^T \\vec{\\beta}) = \\max(0, 1 - y_i (K_i^T \\vec{\\beta}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial basis function kernel.\n",
    "\n",
    "####  The RBF kernel on two samples x and x', represented as feature vectors in some input space, is defined as:\n",
    "\n",
    "## $K(x,x') = \\exp \\big{[}- \\frac{||x-x'||^2}{2 \\sigma^2} \\big{]}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this cell if you are working in colab\n",
    "# ! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_basic/homeworks_basic/assignment0_03_SVM/svm.py -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm import SVM, rbf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y==0] = -1 # for convenience with formulas\n",
    "y_test[y_test==0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVM(epochs=3, lr=1, batch_size=20, verbose=True)\n",
    "clf.fit(X, y)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVM(epochs=100, lr=0.1, batch_size=20, verbose=True, kernel_function=rbf)\n",
    "clf.fit(X, y)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert accuracy_score(y_test, pred) > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model with rbf kernel can learn this dataset too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
